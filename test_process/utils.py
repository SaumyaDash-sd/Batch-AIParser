import os
import threading
import pandas as pd
import time
import logging
import json
from tqdm import tqdm
from dotenv import load_dotenv
from openai import AzureOpenAI

# Load environment variables
load_dotenv()

# Setup logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# Global variable for storing results
output_list = []
output_lock = threading.Lock()


# -------------------------
# GPT Client
# -------------------------
def call_gpt_llm_client(prompt, model: str = "gpt-4o", temperature=0.5, time_delay=65):
    """
    Calls Azure GPT LLM client and handles retries.
    """
    client = AzureOpenAI(
        api_key=os.getenv("azure_openai_api_key"),
        azure_endpoint=os.getenv("azure_endpoint"),
        api_version=os.getenv("azure_openai_api_version"),
    )

    conversations = [
        {"role": "system", "content": prompt},
        {
            "role": "user",
            "content": "Please return only the JSON object. No additional text, no explanations.",
        },
    ]

    try:
        response = client.chat.completions.create(
            model=model,
            messages=conversations,
            temperature=temperature,
        )
        gpt_response = response.choices[0].message.content
        return gpt_response, response.usage
    except Exception as err:
        logging.exception(
            f"Error calling GPT LLM client: {err}. Retrying in {time_delay} seconds..."
        )
        time.sleep(time_delay)
        # Retry once
        response = client.chat.completions.create(
            model=model, messages=conversations, temperature=0.5
        )
        gpt_response = response.choices[0].message.content
        return gpt_response, response.usage


# -------------------------
# GPT Output Parser
# -------------------------
def handling_gpt_output(gpt_response):
    """
    Parses the GPT response, ensuring strict JSON handling.
    """
    try:
        parsed_output = json.loads(gpt_response.strip())
        if not isinstance(parsed_output, dict):
            raise ValueError("Parsed output is not a dictionary.")
        return parsed_output
    except json.JSONDecodeError as e:
        logging.error(f"Failed to parse GPT response as JSON: {e}")
        logging.debug(f"Received response: {gpt_response}")
        return {}


# -------------------------
# Row Processor
# -------------------------
def genai_keyword_category_mapping(
    row, job_title, prompt, placeholder_field, unique_id_column, output_field, chunk_size, credentials
):
    """
    Processes a single row - calls GPT for content generation and handles the response.
    """
    input_col1_name = unique_id_column
    input_col2_name = placeholder_field
    new_col1 = "new_content"
    id_val = row[input_col1_name]
    cat = row[input_col2_name]
    prompt = prompt
    f"""Write informative content for the category = '{cat}'. Create engaging and relevant content. Using keyword research related to '{cat}' to identify high-impact keywords and incorporate them strategically into our content. Write in-depth in 1000 words that provides valuable insights into '{cat}' in <area>,<city> and its relevance to our target audience. Do not use the word keyword anywhere within the content. Avoid using brand names and include topic-related LSI keywords and synonyms. Do not use word keywords, topics, trends and LSI anywhere within the content. do not write anything about brand, manufacturer, importer, and exporters and company in the content. Organize content by including bullet pointers for important information. Enclose each paragraphs by <p> and </p> tags. Enclose each list by <li> and </li> tags. Write SEO-rich content with <area>, <city> after category name in headings and subheadings, with only one <h2> tag at the beginning and the rest as <h3> tags. Ensure the entire content is presented in 3rd person language without using 'we,' 'our,' 'I,' etc. Add <area>, <city> in main heading after category.
    "output": {{
        "new_content": "",
    }}
    """

    gpt_response = None
    token_usage = None

    try:
        gpt_response, usage = call_gpt_llm_client(prompt)
        handled_output = handling_gpt_output(gpt_response)
        token_usage = usage.total_tokens if usage else "N/A"
    except Exception as err:
        logging.exception(f"Error in GPT processing: {err}")
        handled_output = {}

    Term = handled_output.get("new_content", "N/A")

    with output_lock:
        output_list.append(
            {
                input_col1_name: id_val,
                input_col2_name: cat,
                new_col1: Term,
                "token_usage": token_usage,
            }
        )



# import re
# import logging
# from openai import AzureOpenAI

# # ---------------- GPT Client ---------------- #
# def call_gpt_llm_client(prompt, credentials, model: str = "gpt-4o", temperature=0.5, time_delay=65):
#     """
#     Calls Azure GPT LLM client and handles retries.
#     """
#     client = AzureOpenAI(
#         api_key=credentials.get("azure_openai_api_key"),
#         azure_endpoint=credentials.get("azure_endpoint"),
#         api_version=credentials.get("azure_openai_api_version"),
#     )

#     try:
#         response = client.chat.completions.create(
#             model=model,
#             messages=[{"role": "user", "content": prompt}],
#             temperature=temperature,
#         )
#         return response.choices[0].message.content, response.usage
#     except Exception as e:
#         logging.exception(f"Error calling GPT client: {e}")
#         return None, None


# # ---------------- Output Handling ---------------- #
# def handling_gpt_output(gpt_response: str):
#     """
#     Extracts and formats the GPT response.
#     """
#     try:
#         return {"new_content": gpt_response.strip()}
#     except Exception as e:
#         logging.exception(f"Error handling GPT output: {e}")
#         return {"new_content": ""}


# # ---------------- Dynamic Function ---------------- #
# def genai_keyword_category_mapping(
#     row,
#     description_json: dict,
# ):
#     """
#     Processes a single row - dynamically replaces placeholders in the prompt 
#     with values from the dataframe row and calls GPT.
#     """

#     # Extract required fields from description_json
#     job_title = description_json.get("job_title")
#     prompt_template = description_json.get("prompt")
#     unique_id_field = description_json.get("unique_id_field")
#     output_field = description_json.get("output_field", "new_content")  # fallback if not provided
#     chunk_size = description_json.get("chunk_size", 1000)
#     credentials = description_json.get("credentials", {})

#     # Get dynamic placeholders from the prompt (e.g. {cat}, {name})
#     placeholders = re.findall(r"\{(.*?)\}", prompt_template)

#     # Replace placeholders with row values
#     prompt_filled = prompt_template
#     for placeholder in placeholders:
#         if placeholder in row:
#             prompt_filled = prompt_filled.replace(f"{{{placeholder}}}", str(row[placeholder]))
#         else:
#             prompt_filled = prompt_filled.replace(f"{{{placeholder}}}", f"<MISSING_{placeholder}>")

#     # Unique id value for tracking
#     id_val = row[unique_id_field]

#     # Call GPT
#     gpt_response, usage = None, None
#     try:
#         gpt_response, usage = call_gpt_llm_client(prompt_filled, credentials)
#         handled_output = handling_gpt_output(gpt_response)
#         token_usage = usage.total_tokens if usage else "N/A"
#     except Exception as err:
#         logging.exception(f"Error in GPT processing: {err}")
#         handled_output = {"new_content": ""}

#     Term = handled_output.get("new_content", "N/A")

#     # Prepare output dictionary
#     result = {
#         unique_id_field: id_val,
#         output_field: Term,
#         "token_usage": token_usage,
#     }

#     return result



# ðŸ‘‰ Next, in your router, after converting description to JSON and loading the dataframe, you can run like this:

# results = []
# for _, row in dataframe.iterrows():
#     result = genai_keyword_category_mapping(row, description_json)
#     results.append(result)



# -------------------------
# Threaded Execution
# -------------------------
def execute_threads(
    dataframe,
    job_title,
    prompt,
    unique_id_column,
    placeholder_field,
    output_field,
    chunk_size,
    credentials,
    thread_count,
):
    """
    Multithreaded processing of rows.
    """
    threads = []
    progress = tqdm(total=len(dataframe), desc="Processing Rows")

    def worker(rows):
        for _, row in rows.iterrows():
            genai_keyword_category_mapping(
                row,
                job_title,
                prompt,
                unique_id_column,
                placeholder_field,
                output_field,
                chunk_size,
                credentials,
            )
            progress.update(1)

    thread_chunk_size = len(dataframe) // thread_count
    for i in range(thread_count):
        start = i * thread_chunk_size
        end = None if i == thread_count - 1 else (i + 1) * thread_chunk_size
        thread = threading.Thread(target=worker, args=(dataframe.iloc[start:end],))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    progress.close()
    return pd.DataFrame(output_list)


"""
{
    "job_title": "Testing prompt for Keywords",
    "prompt": "Hey GPT, can you check the Department name: {{department_name}} and suggest me whether it is meaningful or not in True or False.\nIf it is not meanignful then suggest a department name related to it.\nAnd can you check the user name: {{users_name}} is it a real name of user or not in True or False, if not real then suggest name also.\nYou have to return your output in below JSON format only, not in any other format:\n{\n\"department_name\": \"Original name of the department\",\n\"meaningful_department_name\": True or False,\n\"suggested_department_name\": \"Suggest related department name\",\n\"original_name\": \"Original name of the user\",\n\"meaningful_user_name\": True or False,\n\"suggested_user_name\": \"Suggest related user name\"\n}\n",
    "placeholder_field": {
      "department_name": "department",
      "users_name": "name"
    },
    "unique_id_field": null,
    "output_field": {
      "department_name": "Original name of the department",
      "meaningful_department_name": "True or False",
      "suggested_department_name": "Suggest related department name",
      "original_name": "Original name of the user",
      "meaningful_user_name": "True or False",
      "suggested_user_name": "Suggest related user name"
    },
    "config": {
      "chunkSize": 20
    },
    "credentials": {
      "endpoint": "https://hari-pulse-abg.openai.azure.com/",
      "apiKey": "openai_api_key_askdsdnsdkadnkas",
      "deploymentName": "gpt-4o-mini",
      "temperature": 0.7
    }
  }

"""


def main_1(
    dataframe,
    job_title,
    prompt,
    unique_id_column,
    placeholder_field,
    output_field,
    chunk_size,
    credentials,
):
    
    # Process with threading
    output_df = execute_threads(
        dataframe,
        job_title,
        prompt,
        unique_id_column,
        placeholder_field,
        output_field,
        chunk_size,
        credentials,
        thread_count=20,
    )

    # Save results
    output_df.to_csv("i.csv", index=False)
    print(
        "âœ… Content generation completed and saved to 'remainining_338_aug_attribute_footer_file.xlsx'"
    )


if __name__ == "__main__":
    dataframe = pd.read_csv("")
    job_title = ""
    prompt = ""
    unique_id_column = ""
    placeholder_field = {}
    output_field = ""
    chunk_size = 20
    credentials = {}
    main_1(
    dataframe,
    job_title,
    prompt,
    unique_id_column,
    placeholder_field,
    output_field,
    chunk_size,
    credentials,
)
